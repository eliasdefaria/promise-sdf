---
title: "Display, Input and Output"
---

SDF allows to specify its

- display formats*, as part of *CLI\* options
- _input file format and location_, as part of an _external table specification_
- _output file format and location_, as part of the _data asset specification_,
  the latter also via the CLI

## **Display Options**

SDF writes progress or results on stdout.

**Commandline options.** There are 5 commandline options to control the output.

- `--quiet` - Do not print `sdf` progress messages.

- `--no-show` - Do not print `sdf` results, i.e., produced tables.

- `--log (info | trace | debug)` - Print logs in various degrees.

  - `info` prints internal progress, SQL statements being processed, logical
    plan being produced
  - `debug` prints more of the above, cache hits, etc
  - `trace` prints internal state changes

- `--print-format (json | csv | yml | tbl)` - Specifies the table representation
  on stdout.

  - `tbl` is the normal ascii table representation.
  - `csv` is a CSV representation
  - `json` is a JSON representation
  - `yml`, only available for `sdf describe`, is YAML representation

- `--limit number` - Limits the number of shown rows. Run with `--limit 0` to
  show all rows.

**Example**: Let's try these options. We start with  
(0) Install

> sdf new . --sample source-to-sink unless you have done so already. Next change
> into that generated directory. Now run the following commands

(1) Usual build

> `sdf build`

(2) Build with limited number of rows; use --limit 0 for all rows

> `sdf build --limit 2`

(3) Build without any progress report

> `sdf build --quiet`

(4) Build without any table info but with progress info

> `sdf build --no-show`

(5) Build without any output except errors

> `sdf build --now-show --quiet`

(6) Build with only trace info

> `sdf build --log trace`
>
> **End of example**

**Controlling which tables will be printed.** `sdf` shows all tables that are
explicitly passed as arguments on the commandline. If you pass no input
argument, it prints all tables of the workspace.

**Example cont'd**:  
(0) Show only the sink table this time via a file

> sdf build src/sink.sql

(1) Show only the `sink` table this time via a target, i.e. a
_catalog.schema.table_ term

> sdf build source_to_sink.public.sink

(2) Of course you can pass as many files or targets as you want. Try

> sdf build src/\*.sql

**End of example**

## **Specifying Input Files**

SDF is **compatible with a variety of different file types**.

| Filetype | Compatibility                                                             |
| -------- | ------------------------------------------------------------------------- |
| csv      | Comma (or other delimiter) separated values, with or without header       |
| parquet  | A self describing, compressed columnar data format, optimized for reading |
| json     | Newline delimited json (also called ndjson)                               |
| gzip     | GZIP compressed json or csv file                                          |
| bzip2    | BZIP2 compressed json or csv file                                         |

**External table specification**. Input locations and file format are currently
specified via `external table` declarations. The external table spec follows
this format:

```sql
create external table T
  <schema>
  stored as <file-format>
  <compression-type>
  <partitions>
  location '<data-location>';
```

We now discus each of the parameters in turn.

### **Schema**

The `schema` of an external table can be given

- **explicitly**, following ANSI standard SQL syntax
- **implicitly**, in which case it is automatically inferred from the data.

An example for an explicit table definition looks like this

```sql
create external table events (
    dt date not null,
    action varchar not null,
    user_id smallint not null)
    ...
```

If the schema is inferred you just write

```sql
create external table events ...
```

Besides column names, types and nullability, SDF schema definitions have no
other schema descriptions, no foreign keys, no constraints.

### **Data Location**

**Local files or directories** can be specified via

- _absolute paths_, like `location 'tmp/data/events.csv'` or
- _relative paths_; like `location 'data/events.csv'`. Relative paths are always
  be _relative to the workspace_.

To specify a _directory_ as input, author it with a trailing `/`, e.g.
`location 'data/'`. Providing a directory assumes that all files with the proper
extension are used as input, see also [partitions](#partitions). below.

Only relative local data files and directories, which are inside the workspace,
can be deployed to the cloud, see [sdf deploy](#sdf-deploy).

**Example**: (1) Let's adapt our running example `source-to-sink` and read data
for `source` from a relative, absolute and remote CSV file.

> `sdf new --sample location-variations-to-sink`

(2) The examples stores the data local to the workspace.

> `cat data/events.csv`

```csv
2022-01-02,mark,1
2022-01-02,trish,4
2022-01-02,mark,2
2022-01-03,trish,1
```

(3) So our sink must use a local path, which is confirmed by inspecting

> `cat src/source.sql`

```sql
create external table T stored as CSV location 'data/';
```

(4) Run the query

> `cat data/events.csv`

(5) Next copy the data to an absolute location like `/tmp/data`.

> `cp -r data /tmp/data`

(6) Change the query location to `'/tmp/data/'`, i.e the new `source should look
like this:

```sql
create external table T stored as CSV location '/tmp/data/';
```

(6) Run the query

> `sdf build`

**End of Example.**

**Remote files and directories.** SDF is designed to read from and write to any
object store. Today it is just AWS S3, but Google Cloud Storage and Azure Blob
Storage are already on the way.

Reading from S3 is trivial; just provide an s3 path, e.g.
`location 's3://bucket/key'`.

Of course, to read and write s3 object store you need permission. To do so set
the following variables:

```
$export AWS_DEFAULT_REGION=us-east-1
$export AWS_REGION=us-east-1
$export AWS_SECRET_ACCESS_KEY=***************************
$export AWS_ACCESS_KEY_ID=**************
```

**Example**:

Let's continue the example `location-variations-to-sink`. Assume we have
`~/.aws_credentials` configured, above AWS environment variables set and have
permissions for creating, reading, writing and listing buckets.

(1) Now, let's create a new bucket test-input-files.

> `aws s3 mb s3://test-output-files`

(2) Next copy the local data to s3:

> `aws s3 cp data s3://test-input-files --recursive`

(3) Change the query location to `'s3://test-input-files/data/'`, i.e the new
`source should look like this:

```sql
create external table T stored as CSV location 's3://test-input-files/data/';
```

(4) Run the query, this time just focussing on the progress messages.

> `sdf build --no-show`

Note that it downloads the data as part of executing the query

```yaml

```

(7) Any subsequent query will only download the remote data again if the remote
data is newer than the last downloaded one. Let's confirm that. To trigger a
rebuild let's touch the sink query.

> `touch src/sink.sql`

(8) Now let's run the query again this time focussing on just the progress
messages.

> `sdf build --no-show`

```yaml

```

**End of Example.**

### **File format**

The file-format can be one of `parquet`, `json`, `ndjson`, or `csv`.

#### **CSV**

CSV is delimited text file that uses a comma to separate values. Each line of
the file will be come a row in a table. A row consists of one or more fields in
plain text, strings or numbers, separated by commas. Each line should have the
same number of fields. A null value is represented by empty text. Some CSV files
also reserve the first row as a header containing a list of field names.

SDF lets you specify CSV separator and header. A typical CSV file format
specification just looks like:

```sql
create external table T  stored as CSV delimiter '|' with row header location 'data/';
```

Drop the delimiter clause if delimiter is ',', drop the row header clause if
your data has no header.

To play with CSV data and the different options, create the CSV example, run
describe, build, change `data/` or change `source.sql`

> `sdf new . --sample csv-variations-to-sink`

#### **Parquet**

Parquet is the SDF recommended file format for working with data.

- It is self describing, i.e. it knows its schema.
- It is a columnar representation, thus efficient for reading.
- It is also storage efficient since it is compressed.

Since Parquet is self decribing, currently SDF forbids to add schema
specification. Parquet external specification just look like so:

```sql
create external table T stored as parquet location 'data/';
```

Using parquet as input data has no further options.

**Example.** Let's use parquet as input and output.

(1) Run

> `sdf new . --sample parquet-to-sink`
>
> (2) Check that the generated directory contains the data `tree data`

```yaml

```

(3) View the raw data, which requires a file path for view to take as argument.

> `sdf view data/events.parquet`

```yaml

```

(4) Run

> `sdf build`

(5) inspect the `sdfcache` which materializes the output:

> `tree data`

```yaml

```

> (6) View the computed result, now we can access the data also via the table
> notation `sdf view parquet-to-sink.public.sink` **End of Example.**

#### **Json**

SDF natively supports newline delimited json [ndjson](http://ndjson.org/).
Working with nested structures is supported with simple dot notation.

**Example.** (1) Let's create a sample project of twitter data:

> `sdf new . --sample json-source-to-sink`

(2) A typical tweet look like this.

> `head 10 data/twitter.json`

```json
{
  "tweet_id": "1223489829817577472",
  "created_at": "Sat Feb 01 06:14:41 +0000 2020",
  "user_id": "3244922072",
  "user_location": {
    "country_code": "th",
    "state": "Saraburi Province"
  },
  "geo": {}
}
```

(3) SDF supports dot-notation and index access to access nested fields and array
elements to make it easy to work with nested structures. So let's read
_user_id_, _country_code_, and _state_ from the _user_location_ object.

> `cat src/sink.sql`

```sql
select
    user_id,
    user_location.country_code as country_code,
    user_location.state as state,
from twitter_json;
```

(4) Run it

> `sdf build`
>
> **End of Example.**

Note: SDF does not yet construct JSON and has no operators beyond `.` and `[]`
at this time.

### **Compression**

CSV and Json are text formats. They compress well. SDF supports two compression
types:

- `GZIP`: Using GZIP compression is quiet useful for any larger text file,
  wether CSV or Json.
- `BZIP2`: At the expense of a higher memory footprint and a slightly longer
  compression time than GZIP, BZIP2s archives are typically >10% smaller than
  with GZIP.

Whether GZip or BZip, SDF let's you simply read compressed data by specifying a
compressed type, e.g. to read BZIP2 data specify an external table in this way.

```sql
create external table events
  stored as CSV
  compression type BZIP2
  location 'data/';
```

To play with CSV data the different compression options, create the compression
example, run describe, and build.

> `sdf new . --sample compression-variations-to-sink`

### **Partitions**

SDF support _Hive partitions_. Hive partitions split a large table into several
smaller parts based on one or multiple columns. In SDF _all partition columns_
are of type `varchar not null`\*.

**Example.** Let's look at partitioned input data.

(1) Install

> sdf new . input-partitions-to-sink

Let's inspect the `events` table, which is here partitioned by day (column `dt`)
and hour (column `hr`)

> `cat src/events.sql`

```sql
create external table events (
    action varchar not null,
    user_id smallint not null)
  )
  stored as csv
  partitioned by (dt, hr)
  location 'data/';
```

The `events` table has four columns: two regular ones `action` and `userid`, and
two partition columns `dt` and `hr`. Observe that partition columns are dropped
from the schema, since their type and nullability is be definition `varchar` and
`not null`.

(3) Hive and thus also SDF maps partitions to directory structures. But how do
directories look like? Assume we have a full day of events for Jan, 2nd, 2022,
where events are grouped every 6 hours, e.g at 00, 06, 12, and 18 hours. Then a
Hive partitioning scheme look like our `data/ directory.

> `tree data`

```yaml
data └─── dt=2022-01-02 ├── hr=00 │   └── data.csv ├── hr=06 │   └── data.csv
├── hr=12 │   └── data.csv └── hr=18 └── data.csv
```

We see that the events table makes every `dt` and `hr` value into a directory,
where

- `dt` is formatted using the strftime format "%Y-%m-%d"
- `hr` is formatted using the strftime format "%H"

and we have a new partition `every 6 hours`. We call the values 2022-01-02, and
00, 06 etc. the _partition keys_!

(4) Users still access the data from just one table. Where clauses restrict
access to sub-tables.

- Read all partitions
  - `select * from events`
- Read a single day partition
  - `select * from events where dt='2022-01-02'`
- Read a single hr partition
  - `select * from events where dt='2022-01-02' and hr='00'`
- Read all hr partitions between hr=00 and hr=12:
  - `select * from events where dt='2022-01-02' and hr between '00' and '12'`

The actual `sink.sql` is the second alternative.

> `cat src/sink.sql`

```sql
select * from events where dt='2022-01-02'
```

(5) Run it

> `sdf build`

**End of Example**

Note, currently SDF allows you to write only conjunctions as filter predicates.

SDF also supports non-time based input partitions, e.g. you might have split the
data by country or organization. And it does the same for output partitions
which will describe in the next subsection.

## **Specifying Output Files**

SDF outputs have less flexibility than its inputs.

### **Data Location**

SDF outputs are only available via its cache or external location.

**Local files or directories** By default SDF writes all table outputs in
`parquet` format to its local cache at
`.sdfcache/[[env]/]/<target>[/<partition>]/<file>.<suffix>`

- `env` is the selected workspace environment.
- `target`, is the fully fully qualified table name `catalog/schema/table`
- `partition` if any, is the Hive partitioning representation
- `file` is a system generated file name,
- `suffix` is the file format suffix.

**Remote files or directories**

To specify that an output file should stored on the cloud, you have to specify
the `--location` option on the command line. The option takes an additional
string argument:

- -- `location` `s3://bucket`
  - This stores all produced target tables under
    - `s3://bucket/[[env]/]/<target>[/<partition>]/<file>.<suffix>`
- -- `location` `remote`
  - This is equivalent to the above form, except that it takes the `s3://bucket`
    location from the `remote-location` field in `workspace.yml`

**Example contd.** Let's store the materialized target tables of any of our past
projects on s3. Of course this requires that we have `~/.aws_credentials`
configured to have permissions for creating, reading, writing and listing
buckets.

(1) Let's create a new bucket test-output-files.

> `a`ws s3 mb s3://test-output-files`

(2) Let's run `sdf`

> `sdf build --remote s3://test-output-files`

(3) Let's check that the data landed on s3:

> `aws s3 ls s3://test-output-files --recursive`

```yaml
2023-03-06 14:06:34        869 output_files/public/main/part-0.parquet
```

(4) Let's compare the file object size with the size of the local data

> `ls -lR .sdfcache | grep part-0.parquet`

```yaml
-rw-r--r--@ 1 wosch  staff  869 Mar  6 14:06 part-0.parquet
```

(5) But what happens if we run build again? let's focus just on the progress
report.

> `sdf build --remote s3://test-output-files --no-show`

```yaml

```

It says that the cloud is up-to-date and thus no file is pushed to the cloud.
**End of Example.**

### **File Format**

**File Format.** SDF's preferred file format is `parquet`. You can override it
by using a

- the CLI option -- `file-format` `csv` or `json`
  - This will generate the requested format for all materialized tables.
- a `das.yml` specification.
  ```yaml
  schemas:
    - schema:
        name: output_files.public.main
        file-format: csv
  ```
  - This will generate the requested format just for the one schema you are
    interested in.

Note that the new format is only materialized if the table needs to be
regenerated, run `sdf` `clean` to force regeneration of tables.

**Example contd**.

(1) Let's run

> `sdf` `build` `--file-format` `csv` We see that it produced `cat
> .sdfcache/output_files/public/main/part-0.csv

```yaml
....
```

**End of Example.**

### **Compression**

SDF does **not** support compression for its output. By _using parquet_ you get
already the best compression possible.

### **Partitions**

SDF supports _Hive output partitions_ as well. We will discuss this in the next
section on scheduling and partitions.
