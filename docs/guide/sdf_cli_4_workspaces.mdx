---
title: "SDF Workspaces"
description:
  "In this document, we're going to discuss a critical part of the SDF ecosystem
  - the workspace."
---

If you've worked with Javascript, this might remind you of a `package.json`. In
Rust, a `Cargo.toml`. This file will define the configuration for your project,
and will be used by SDF to build and deploy your project. In SDF, this file is
called `workspace.sdf.yml`. This file is required in your project and sdf will
fail without it.

<Note>
  The `workspace.sdf.yml` is actually a special instance of the general
  [sdf.yml](/reference/sdf-yml), which we'll discuss further below.
</Note>

## sdf.yml

As previously mentioned, the `workspace.sdf.yml` is a specific instance of an
`sdf.yml` file. High-level, `sdf.yml's` are general-purpose metadata descriptors
for describing things like:

- Where your queries should look for input data
- How that data is represented
- The SQL dialect your query is written in
- The schedule on which your query should run
- And more...

The `sdf.yml` is structured such that there are a set of specific YAML blocks
titled by their purpose in your project. For example, you might have a `table`
block, a `classifier` block, and a `policy` block all in the same file. Each of
these blocks will contain a set of properties that define the metadata for that
block. The types of metadata differ per block type, but they all share a few
properties.

`sdf.yml` files can be placed anywhere within your SDF project, and can be named
whatever you like, as long as the file extension is `.sdf.yml`. For example, you
could have a `tables.sdf.yml` file, a `classifiers.sdf.yml` file, and a
`policies.sdf.yml` file. Or you could have an `sdf.yml` per table defined in
your project. Maybe I have a table `six_hourly_ingest.sql` and a corresponding
`six_hourly_ingest.sdf.yml`. The possibilites are truly endless.

<Tip>
  Our engine supports multiple dialects in the same SDF project. That means even
  column-level lineage will work across SQL dialects
</Tip>

#### Shared Properties Among All Metadata Blocks

`name` - The name of the metadata block. Names must be unique within their block
type. For example, two tables cannot have the same name, but a table and a
classifier could share the name `pii`.

`description` - This is a free-form text field that can be used to describe the
metadata block. This is useful for documentation purposes.

<Tip>
  All this metadata is indexed and searchable in the SDF Console (coming
  soon...)
</Tip>

## Metadata Blocks

Here we'll briefly review each of the metadata block types. Each serves their
own purpose, and are used in different ways. For a full list of the options
available in the `sdf.yml`, see the [sdf.yml](/reference/sdf-yml) reference.

### Workspace Block

The workspace block is a special metadata block that belongs in your
`workspace.sdf.yml`. This defines your workspace configuration, and is used by
the SDF engine to build and deploy your project. For more information on the
workspace block, see the
[Workspace Block](/reference/sdf-yml#root-element-workspace) reference.

#### Hello World Workspace

The simplest version of a `workspace.sdf.yml` looks like this:

```yaml
workspace:
  edition: 1
  includes:
    - path: src
```

So what's going on here? The first thing you'll notice is the workspace
specifies an `edition`. This tells the SDF engine which version of the workspace
to expect when compiling your project.

Next we see an `includes` block. This tells the SDF engine where to look to find
the code sources it will use to build your project and transform your data.
Simply put, your SQL files should go into the `src` directory.

For a full list of the options available in the `workspace.sdf.yml`, see the
[sdf.yml](/reference/sdf-yml) reference.

#### Advanced Workspace Configurations

Let's say a you're developing a data pipeline that needs to run on two different
environments - a staging environment for testing updates and a production
environment for live data processing. To configure these environments with SDF,
you can create two workspace files: `staging.workspace.sdf.yml` and
`production.workspace.sdf.yml`.

By including the prefix `staging` or `production` before `workspace.sdf.yml`,
SDF will create a default catalog within the project with the corresponding
name. The user can then specify different settings and parameters for each
environment within the workspace file. For example, you might want to use
different data sources or databases for staging and production, or you might
want to set different data processing rules or data quality checks.

One more use case for configurations would be for breaking up your project into
smaller executable units. Imagine you're developing a large data project that
includes multiple data sources and data processing steps. To make the project
more manageable, you might want to break it up into executable units based on
different configurations. For example, you might want to have one configuration
for data ingestion and one for data processing.

To configure these different units, you can create multiple workspace files:
`ingestion.workspace.sdf.yml` and `processing.workspace.sdf.yml`. Within each
workspace file, you can specify a different set of included paths - i.e. the
files and directories that are used for that specific configuration. You can
then build, describe, and deploy each configuration separately with the commands
below:

```bash
sdf build --config ingestion
```

```bash
sdf describe --config processing
```

#### Important Properties

```yaml
workspace:
  name: my_project
  edition: 1
  includes:
    - path: src/*
  default-catalog: my_project
  default-schema: public
  remote-location: s3://[MY-BUCKET]/my-project/
  dialect: sdf
  references:
    - path: ../other_project
```

- `name` - The name of the workspace. This is used to create the default catalog
  for the workspace.
- `edition` - The edition of the workspace. This is used to determine which
  version of the workspace to expect when compiling your project.
- `includes` - The includes block defines where to look for the code sources
  that will be used to build your project and transform your data.
- `default-catalog` - The default catalog is the catalog that will be used when
  no catalog is specified by a metadata block
- `default-schema` - The default schema is the schema that will be used when no
  schema is specified by a metadata block.
- `remote-location` - The default output object store location, e.g.
  's3://bucket/key/' where key is optional. This is used to determine where to
  write the output of your project.
- `dialect` - The default SQL dialect that will be used to build your project.
  SDF supports multi-dialect projects, so you can have different SQL dialects in
  the same project by configuring the dialect in the
  [table metadata block](#table-block). However, most projects will only use
  one. If unspecified, the default dialect is `sdf`, an ansi-compliant standard
  SQL.
- `references` - An array of paths to other workspaces. This enables
  cross-workspace dependencies and references between them.

### Classifier Block

The classifier block is used to define classifiers in your SDF projects. You can
think of these as units of metadata to be reused and measured across your data
warehouse. Classifiers have names, descriptions, and even namespaces and
subsets. For more detailed information on the classifier block, see the
[Classifier Block](/reference/sdf-yml#nested-element-classifier) reference.

In light of data privacy regulations, a common use case for classifiers is to
define PII columns. You can define a classifier for PII columns, then apply that
classifier to any column in your project and watch it propagate automatically to
downstream queries. This will allow you to easily identify PII columns across
your data warehouse, and even enforce data governance checks on them in the
future.

Here's an example of a basic PII classifier:

```yaml
classifier:
  name: pii
  description: Personally Identifiable Information
  elements:
    - name: email
      description: Email address
    - name: phone
      description: Phone number
    - name: ssn
      description: Social Security Number
```

This classifier can be used across columns and tables alike to tell the SDF
engine where to find `PII`. See the [Table Block](#table-block) section for an
example of how to apply this classifier to a column.

<Tip>
  SDF automatically propagates classifiers to downstream dependencies. This
  helps you keep your metadata up to date and consistent across your entire data
  warehouse without having to manually update it everywhere. This makes it
  easier to stay compliant with data privacy regulations like GDPR and CCPA.
</Tip>

### Table Block

The table block is used to define the metadata for a table. This includes the
table name, the table description, the table's columns, and even the table's
schedule and SQL dialect. This will likely be your most widely used metadata
block. For more detailed information on the table block, see the
[Table Block](/reference/sdf-yml#table) reference.

#### Column-Level Classifiers

One of the most important use cases for the table block is defining column-level
classifiers. SDF will propagate these classifiers to downstream dependencies
automatically. Here's an example of a basic table definition with a column-level
classifier:

```yaml
table:
  name: main
  columns:
    - name: phone
      classifiers:
        - pii.phone
```

This table definition will tell SDF that the column `phone` is a `pii` column.

#### Table-Level Classifiers

This is great, but what if you have a table with 100 columns, and you want to
define a classifier for all of them? In that case we can apply the classifier at
the table level. Here's an example of a table definition with a table-level
classifier:

```yaml
table:
  name: main
  classifiers:
    - pii
```

#### Data Modeling

Classification is a powerful feature of SDF, but it's not the only thing you can
do with the table block. You can also define the SQL dialect for your table, the
materialization, and even partitioning information. For example, you might have
a table that is defined in a SQL file, but you want to materialize it as a table
and tell SDF how it's partitioned. You can do that with the `materialization`
and `partitioned_by` properties:

```yaml
table:
  name: partitioned_table
  dialect: spark
  materialization: cached
  partitioned-by:
    - name: "dt"
      format: "%Y%m%d"
    - name: "hr"
      format: "%H"
```

In this example, we partition a column named `dt` by date and expect the date
string to be formatted in the order of year month and day (e.g. 2022-01-01).
Furthermore, we partition a column named `hr` by hour (e.g. 2). Finally, we
materialize it as a table, but in SDF we call this `cached` as SDF will
automatically cache the table and only re-run it if new data is available.

For a full list of supported materializations, dialects and paritioning formats,
see [Materializations](/reference/sdf-yml#nested-element-materialization),
[Dialects](/reference/sdf-yml#nested-element-dialect), and
[Partitioning](/reference/sdf-yml#nested-element-partitioned-by) in the
reference.

You might be wondering where the SQL for your table is defined. It's defined in
the SQL file that you include in your `includes` block. For example, if you have
a table named `main`, you might have a SQL file named `main.sql` in your `src`
directory. SDF will automatically find this file and use it to build your
project. Here's an example of a table definition with a SQL file:

```yaml src/popdata.sdf.yml
table:
  name: popdata
  format: csv
  header: true
  location: local/pop.csv
```

```sql src/popdata.sql
create external table popdata
stored as csv
with header row
location 'local/pop.csv';
```

Lastly, it's worth mentioning the sheer amount of work SDF does for you. _Most
of your table blocks will end up being generated at compile-time by SDF_. Things
like column datatypes, propagated classifiers, and column-level lineage will be
automatically generated for you and structured into a table metadata block. SDF
does this by statically analyzing your SQL files and extracting this
information. This is a powerful feature of SDF, and it's one of the reasons why
it's able to provide so much value with so little effort from the developer.

Here's an example of a full-fledged table definition:

```yaml .sdfcache/assembly.sdf.yml
table:
  name: default.public.main
  includes:
    - path: src/main.sql
  dialect: sdf
  columns:
    - name: time_date
      datatype: timestamp
      nullable: false
    - name: user_id
      datatype: varchar
      nullable: false
      lineage:
        copy:
          - j_table.user_id
          - b_table.user_id
          - i_table.user_id
          - k_table.user_id
          - h_table.user_id
          - f_table.user_id
          - g_table.user_id
          - d_table.user_id
          - e_table.user_id
          - c_table.user_id
          - l_table.user_id
          - m_table.user_id
          - a_table.user_id
    - name: user_id_hash
      datatype: int
      nullable: false
      lineage:
        transform:
          - j_table.user_id
          - b_table.user_id
          - i_table.user_id
          - k_table.user_id
          - h_table.user_id
          - f_table.user_id
          - g_table.user_id
          - d_table.user_id
          - e_table.user_id
          - c_table.user_id
          - l_table.user_id
          - m_table.user_id
          - a_table.user_id
```

### Catalog Block

The catalog block is used to define the metadata for a catalog. We use the term
catalog in correspondence with the commonplace terminology to describe catalogs
and schemas in database systems. See the diagram below for how these fit
together.

The most basic catalog block would look something like this:

```yaml
catalog:
  name: default
  includes:
    - src/default/
```

<div style={{ width: "100%", display: "flex", justifyContent: "center" }}>
  <img
    src="https://cdn.sdf.com/img/sdf-catalog-schema-database-chart.svg"
    width="50%"
  />
</div>

### Schema Block

The schema block is used to define the metadata for a schema. A schema is a
subset of a catalog. One catalog can have multiple schemas.

The most basic schema block would look something like this:

```yaml
schema:
  name: public
  includes:
    - src/default/public
```

### The Assembly File

The assembly file is the output of a project. It's how SDF represents all the
metadata about your project when you decide you're ready to deploy. After
running an SDF command, you'll find a new directory appears called `.sdfcache`.
This is automatically added to your .gitignore and should not be tracked in git.
As the name implies, this folder enables SDF to cache intermediaries as you're
building your project locally. Along with its caching capability, it also
generatesa file after running any SDF command called `assembly.sdf.yml`. Here
lies the fated assembly file.

The assembly file aggregates all the `sdf.yml`s across your project and places
them into a single file. Along with the developer-authored `sdf.yml` files and
properties, the assembly file also contains all the metadata generated by SDF.
This metadata is used to enable features like column-level lineage, classifier
propagation, and more.

The assembly file is a powerful representation that can help you understand what
SDF is doing under the hood. You can use the assembly file to understand how SDF
is interpreting your project, and you can also use it to debug issues that might
arise. We hope developers will take this representation and build their own
tools and functionalities on top of it in the future.

<Tip>
  Want to build something cool that utilizes the SDF engine? We'd love to hear
  about it! [Reach out to us](mailto:info@sdf.com) (info@sdf.com) and let us
  know what you're working on and how we can help.
</Tip>

## Future Looking

These future looking features are not yet available, but we're excited to share
our concrete plans to roll many of them out. Sign up for our
[email list](https://sdf.com) to stay up to date on our progress.

### Policy Block

We'll soon be introducing a Policy Block to SDF. This block will allow you to
define data use, access, and retention policies across your SDF project.
Policies will operate on
[Classifiers](/reference/sdf-yml#nested-element-classifier) and
[Tables](/reference/sdf-yml#nested-element-table). For example, you might want
to define a policy that say that all columns with the `pii` classifier must be
masked before being used in any downstream tables. Your team's IDP can be
connected here as well in order to enforce Role Based Access Control (RBAC) and
Attribute Based Access Control (ABAC) policies with your internal stakeholders.
Even better, we'll be one of the few tools that enable you to also define
Purpose Based Access Control (PBAC) policies for use cases like GDPR.

### Function Block

We'll also soon be introducing a Function Block to SDF. This functionality will
enable you to define in code how a SQL function operates on a
[Classifier](#classifier-block). For example, let's say we have a function
called `mask_pii` that masks the value of column and replace it with `***`. In
this case, we want the Classifier `pii` to be propagated to the output column
with a signifier that it's been masked, something like `pii::masked`. SDF needs
to know what happened to the data in order to properly propagate classifiers and
lineage. The function block will allow you to define this behavior in code.

### Assertion Block

Data quality is something we've seen many teams struggle with. We'll soon be
introducing an Assertion Block to SDF. This block will allow you to define
assertions about your data. Think of these like tests you can run on your data
to ensure simple things like a column does not contain null values, or more
complex cases like a column is within a certain range of values.
